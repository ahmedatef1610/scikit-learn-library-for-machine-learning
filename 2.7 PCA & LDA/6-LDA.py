# Import Libraries
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

from sklearn.datasets import make_blobs, make_classification , load_iris
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

import numpy as np
import pandas as pd

import seaborn as sns
import matplotlib.pyplot as plt
# ----------------------------------------------------
# Linear Discriminant Analysis
# A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes’ rule.
# The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix.
# The fitted model can also be used to reduce the dimensionality of the input by projecting 
# it to the most discriminative directions,using the transform method.

'''
class sklearn.discriminant_analysis.LinearDiscriminantAnalysis(solver='svd', shrinkage=None, priors=None, n_components=None, 
                                                                store_covariance=False, tol=0.0001, covariance_estimator=None)
===
    - solver{‘svd’, ‘lsqr’, ‘eigen’}, default=’svd’
        Solver to use, possible values:
        - ‘svd’: Singular value decomposition (default). Does not compute the covariance matrix, 
            therefore this solver is recommended for data with a large number of features.
        - ‘lsqr’: Least squares solution. Can be combined with shrinkage or custom covariance estimator.
        - ‘eigen’: Eigenvalue decomposition. Can be combined with shrinkage or custom covariance estimator.
    - shrinkage‘auto’ or float, default=None
        Shrinkage parameter, possible values:
            None: no shrinkage (default).
            ‘auto’: automatic shrinkage using the Ledoit-Wolf lemma.
            float between 0 and 1: fixed shrinkage parameter.
        This should be left to None if covariance_estimator is used. Note that shrinkage works only with ‘lsqr’ and ‘eigen’ solvers.
    - priors array-like of shape (n_classes,), default=None
        The class prior probabilities. By default, the class proportions are inferred from the training data.
    - n_components int, default=None
        Number of components (<= min(n_classes - 1, n_features)) for dimensionality reduction. 
        If None, will be set to min(n_classes - 1, n_features). 
        This parameter only affects the transform method.
    - store_covariance bool, default=False
        If True, explicitely compute the weighted within-class covariance matrix when solver is ‘svd’. 
        The matrix is always computed and stored for the other solvers.
    - tol float, default=1.0e-4
        Absolute threshold for a singular value of X to be considered significant, used to estimate the rank of X. 
        Dimensions whose singular values are non-significant are discarded. Only used if solver is ‘svd’.
    - covariance_estimator covariance estimator, default=None
        If not None, covariance_estimator is used to estimate the covariance matrices instead of relying on 
            the empirical covariance estimator (with potential shrinkage). 
            The object should have a fit method and a covariance_ attribute like the estimators in sklearn.covariance. 
            if None the shrinkage parameter drives the estimate.
        This should be left to None if shrinkage is used. Note that covariance_estimator works only with ‘lsqr’ and ‘eigen’ solvers.
===


'''
# ----------------------------------------------------
print("="*25)

IrisData = load_iris()
X = IrisData.data
y = IrisData.target
y_hue = pd.Series(y)
y_hue[y == 0] = IrisData.target_names[0]
y_hue[y == 1] = IrisData.target_names[1]
y_hue[y == 2] = IrisData.target_names[2]
print(X.shape, IrisData.feature_names)
print(y.shape, IrisData.target_names)
# print(y)
# print(y_hue.values)
print("="*10)
# ---------
scaler = MinMaxScaler(copy=True, feature_range=(0, 1))
X = scaler.fit_transform(X)
# ---------
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, random_state=16, shuffle=True)
print(X_train.shape,X_test.shape)
print(y_train.shape,y_test.shape)
print("="*25)
# ---------
# Applying LDAModel Model
LDAModel = LDA(n_components=2, solver='svd')
X = LDAModel.fit_transform(X,y)
# X_train = LDAModel.fit_transform(X_train,y_train)
# X_test = LDAModel.transform(X_test)
print(X.shape)
print("="*10)
# ---------
scaler = MinMaxScaler(copy=True, feature_range=(0, 1))
X = scaler.fit_transform(X)
# ---------
# ----------------------------------------------------
# Calculating Details
print('LDAModel Train Score is : ', LDAModel.score(X_train,y_train))
print('LDAModel Test Score is : ', LDAModel.score(X_test,y_test))
print("="*10)
# ----------------------------------
print('LDAModel Weight vector(s) : ', LDAModel.coef_)
print('LDAModel Intercept term : ', LDAModel.intercept_)
print('Scaling of the features in the space spanned by the class centroids : ', LDAModel.scalings_)

# print('LDAModel singular value is : ', LDAModel.covariance_) # only exists when store_covariance is True.
print('LDAModel Explained Variance is : ', LDAModel.explained_variance_ratio_)
print('Overall mean : ', LDAModel.xbar_)
print('LDAModel Class-wise means is : ', LDAModel.means_)
print(' Class priors (sum to 1) : ', LDAModel.priors_)
print('Unique class labels : ', LDAModel.classes_)
print("="*25)
# ----------------------------------------------------
y_pred = LDAModel.predict(X_test)
# y_pred_prob = LDAModel.predict_proba(X_test)
# print('Pred Probabilities Value for LDAModel is : ', y_pred_prob[:5])
print('Pred Value for LDAModel is : ', y_pred[:5])
print('True Value for LDAModel is : ' , y_test[:5])
print("="*10)
# ----------------------------------------------------
# x_axis = np.arange(0-0.1, 1+0.1, 0.001)
# xx0, xx1 = np.meshgrid(x_axis,x_axis)
# xxx = np.concatenate((np.ones((xx0.shape[0]*xx0.shape[1],2)),np.c_[xx0.ravel(), xx1.ravel()]),axis=1)
# Z = LDAModel.predict(xxx).reshape(xx0.shape)

plt.figure("iris")
sns.scatterplot(x=X[:,0], y=X[:,1], hue=y_hue, alpha=1, palette=['r','b','k']);
# plt.contourf(xx0, xx1, Z, alpha=0.2, cmap=plt.cm.Paired)
plt.show(block=True) 

# plt.figure("iris")
# sns.scatterplot(x=X_train[:,0], y=X_train[:,1], hue=y_hue, alpha=1, palette=['r','b','k']);
# plt.show(block=True) 
# plt.figure("iris")
# sns.scatterplot(x=X_test[:,0], y=X_test[:,1], hue=y_hue, alpha=1, palette=['r','b','k']);
# plt.show(block=True) 


